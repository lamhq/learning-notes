# Overview

## Three "V" of Big data

- **Volume**: refers to the sheer amount of data that is being generated and collected.
Ranges from terabytes to petabytes of data.
- **Variety**: describes the various types of data
from a wide range of sources and formats.
- **Velocity**: pertains to the speed at which data is collected, stored, processed, and analyzed.


## Data lake

A data lake collects data from multiple sources into one combined repository.

It holds raw, unprocessed data of various types, including structured, semi-structured, and unstructured data.

It is designed to capture and store large amounts of data. They serve as a foundation for machine learning, AI, other analytics. After processing, data from a lake can be transferred to a data warehouse.


## Data Warehouse

A data warehouse is designed to store processed and refined data.

It serves as a repository for structured data that can be queried and analyzed for specific purposes.


## ETL

ETL (Extract, Transform, Load) is a process of extracting data from different source systems, transforming it to meet specific business requirements, and loading it into a target database or data warehouse for further analysis and reporting.

ETL processes are important for gaining valuable insights and making critical business decisions.
